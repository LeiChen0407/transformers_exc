{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee13149",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### tokenizer理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ccc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download --resume-download google-bert/bert-base-uncased --local-dir ../../model/bert-base-uncased --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0fca718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\anaconda\\envs\\llm\\lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed49910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"../../model/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(\"cuda\")\n",
    "sentences = \"this is a test sentence\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1ae14",
   "metadata": {},
   "source": [
    "### model\n",
    "1.模型详细结构\n",
    "- model\n",
    "\n",
    "2.模型整体结构\n",
    "- model.config\n",
    "\n",
    "3.模型参数\n",
    "- model.num_parameters()\n",
    "- eps 相当于$\\epsilon$\n",
    "- elementwise_affine 相当于bias\n",
    "$$\\mathrm{output}=\\mathrm{weight}\\cdot\\frac{\\mathrm{input}-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}+\\mathrm{bias}$$\n",
    "\n",
    "- model.norm.weight :参考norm层详细的参数\n",
    "\n",
    "### model(**tokens)\n",
    "1.model(**tokens) 解析\n",
    "- 输出得到last_hidden_state、pooler_output\n",
    "- last_hidden_state维度:(batch_size,sequence_length,hidden_size)\n",
    "- pooler_output维度:(batch_size,hidden_size)\n",
    "- 可将两者用于下游任务，例如分类等任务\n",
    "- pooler_output == model.pooler(last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "65166e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2572, 2061, 3407,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"i am so happy\"\n",
    "tokens = tokenizer(sentences, truncation = True,padding=True,max_length=256,return_tensors=\"pt\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fde148c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'this', 'is', 'a', 'te', '##set', 'sentence', '[SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(sentences)\n",
    "tokenizer.encode(sentences)\n",
    "tokenizer.decode([101, 2023, 2003, 1037, 8915, 13462, 6251, 102])\n",
    "tokenizer.convert_ids_to_tokens([101, 2023, 2003, 1037, 8915, 13462, 6251, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca53eeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2023, 2003, 1037, 3231, 6251]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids([ 'this', 'is', 'a', 'test',  'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adfb7345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2023, 2003, 1037, 3231, 6251]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(sentences.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "401d69a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map.values()\n",
    "tokenizer.convert_tokens_to_ids(tokenizer.special_tokens_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0bdd127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff06790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**tokens)\n",
    "with torch.no_grad():\n",
    "    output = model(**tokens)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89d7d7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f9c6c66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3806,  0.5056]]), hidden_states=None, attentions=None)\n",
      "tensor([[0.2919, 0.7081]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "sequence_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = sequence_model(**tokens)\n",
    "    softmax = output.logits.softmax(dim=-1)\n",
    "    prediction = torch.argmax(softmax,dim=-1)\n",
    "\n",
    "    print(output)\n",
    "    print(softmax)\n",
    "    print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "88de1aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LABEL_0': 0, 'LABEL_1': 1}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_model.config.label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247e5a6",
   "metadata": {},
   "source": [
    "### tokenizer\n",
    "1.查看分词情况:\n",
    "- tokenizer.tokenize(sentences)\n",
    "\n",
    "2.编码解码\n",
    "- tokenizer.encode(sentences) = tokenizer.convert_tokens_to_ids()\n",
    "- tokenizer.decode(senteencs) = tokenizer.convert_ids_to_tokens()\n",
    "\n",
    "3.特殊编码\n",
    "- tokenizer.special_tokens_map\n",
    "\n",
    "4.词汇表\n",
    "- tokenizer.vocab\n",
    "\n",
    "5.输出解析\n",
    "- ipute_ids:对输入进行vocal表的映射，即将自然语言转换成模型理解的语言\n",
    "- attention_mask:可以理解为对词的关注\n",
    "    - 如果有很多句子，len(attention_mask)等于最长句子的长度，目的是为了能够批次化输入，确保所有句子的输入长度都是一样的，跟padding=True相结合理解\n",
    "- token_type_ids:用于区分不同的句子，len(token_type_ids)==len(attention_mask),一般用于句子对,相应的任务,可以理解为用于预测下一个句子是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04ca3b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"this is a test sentence\", \n",
    "             \"i am so happy\",\n",
    "             \"test sentence\",\n",
    "             \"i am so sad,that so sad\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64cd961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(sentences, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2b8bb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1037, 3231, 6251, 102, 1045, 2572, 2061, 3407, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(text=sentences[0],\n",
    "                      text_pair=sentences[1],)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c1f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
