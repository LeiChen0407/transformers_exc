{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22fd746f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\anaconda\\envs\\llm\\lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,AutoConfig,AutoModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb1324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"../../model/gpt2\"\n",
    "CLLM = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "llm = AutoModel.from_pretrained(model_name, trust_remote_code=True,output_hidden_states=True,output_attentions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1425a26f",
   "metadata": {},
   "source": [
    "### AutoModelForCausalLM 跟 AutoModel的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab19e40",
   "metadata": {},
   "source": [
    "#### AutoModelForCausalLM 跟 AutoModel的区别\n",
    "- 多了一个MLPHead,维度为(outputsize,vocal_size)\n",
    "- 即输出到词表的映射\n",
    "\n",
    "#### gpt2属于自回归模型\n",
    "- 对于输入x,得到$y_1$,然后将[x,$y_1$]作为输入,接着得到$y_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85db5aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53031f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeacb69a",
   "metadata": {},
   "source": [
    "### greed_search vs. beam_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b80232",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "#### greed_search vs. beam_search\n",
    "- greed_search:贪心策略，对于每次的输入y,总是选择概率最大的那个,弊端：输出缺乏多样性,场景：数学(要求精确，而不是多样)\n",
    "- beam_search:束搜索，输入的时候进行束(k)展开，每次输出都进行k展开，最后计算得到最大概率的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17576636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def predict_next_tokens(model, tokenizer, sentence, num_steps=10, top_k=5):\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.clone()  \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            probs = torch.softmax(logits[:, -1, :], dim=-1, dtype=torch.float32)\n",
    "            \n",
    "            top_probs, top_indices = torch.topk(probs, top_k, dim=-1)\n",
    "            \n",
    "            step_result = {\n",
    "                '步骤': f'第{step+1}步',\n",
    "                '当前句子': tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            }\n",
    "            \n",
    "            for i in range(top_k):\n",
    "                token_id = top_indices[0][i].item()\n",
    "                probability = top_probs[0][i].item() * 100\n",
    "                token_text = tokenizer.decode([token_id])\n",
    "                \n",
    "                step_result[f'Top{i+1}_Token'] = token_text\n",
    "                step_result[f'Top{i+1}_概率(%)'] = f'{probability:.2f}%'\n",
    "            \n",
    "            results.append(step_result)\n",
    "            \n",
    "            next_token_id = top_indices[0][0].unsqueeze(0).unsqueeze(0)\n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_prediction_results(df):\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Token预测结果\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', 30)\n",
    "    \n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def run_token_prediction(model, tokenizer, sentence, num_steps=10, top_k=5):\n",
    "\n",
    "    try:\n",
    "        results_df = predict_next_tokens(model, tokenizer, sentence, num_steps, top_k)\n",
    "        formatted_df = display_prediction_results(results_df)\n",
    "        return results_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"预测过程中出现错误: {e}\")\n",
    "        return None\n",
    "\n",
    "df_result = run_token_prediction(CLLM, tokenizer, \"hello,how are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c0e38",
   "metadata": {},
   "source": [
    "### logits的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857ae47a",
   "metadata": {},
   "source": [
    "- 对任意句子(i have a )，进行tokenizer编码后(假设不包含special_token)，得到input_ids,长度为tokens_len\n",
    "- 将input_ids输入到model中,即output = model(input_ids=input_ids),\n",
    "- 得到output.logits,维度为(batch_size,tokens_len,vocab_size)\n",
    "- 对于非最后一个token,即(batch_size,tokens_len[i],vocab_size),i!=-1,输出的含义:model更加偏向对该位置token的预测,\n",
    "    - 例如(batch_size,tokens_len[0],vocab_size),是指model对第一个token(i)的预测概率的一个排序\n",
    "- 而对于最后一个token,即(batch_size,tokens_len[-1],vocab_size),输出的含义:model更加偏向对下一个token的预测\n",
    "    - 例如(batch_size,tokens_len[-1],vocab_size),更多的是对apple这个token的预测，而不是a这个token的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84598c51",
   "metadata": {},
   "source": [
    "### 相关函数解释"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b5e17",
   "metadata": {},
   "source": [
    "- torch.argsort(),返回张量排序后的索引（indices）\n",
    "    - 例如：lst = [3.1,-0.8,2.5],torch.argsort(lst),得到[1,2,0](升序顺序)\n",
    "    - torch.argsort(lst,descending=True),得到[0,2,1](降序顺序)\n",
    "- torch.argmax(),返回张量顺序后最大的索引\n",
    "    - torch.argmax(lst),得到[0]\n",
    "- model.generate()\n",
    "    - max_length:prompt+generation的长度\n",
    "    - max_new_tokens:generation的长度\n",
    "    - 默认greed_search,如果beam_num不设置的话\n",
    "- model.wte(input.input_ids)\n",
    "    - 得到每一个token的编码后的向量\n",
    "- model.wpe.weight\n",
    "    - 得到位置编码向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba778ee9",
   "metadata": {},
   "source": [
    "### 理解tokenizer.decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10718806",
   "metadata": {},
   "source": [
    "#### 如何理解tokenizer.decode()的时候，是tokenizer.decode(torch.argmax(lst)),是decode的索引,而不是logits\n",
    "- 对于model(input_ids = input_ids).logits[:,-1,:],得到的是模型对于vocab再该位置输出token的概率\n",
    "- 其中logits[:,-1,:]潜在的包含了顺序的关系，可以理解为logits[:,-1,:][0],表示的是对于词汇表token ids为0对应的token的概率\n",
    "- 然后经过torch.argmax(lst)得到最大概率的索引,假设为100,那么logits[:,-1,:][100],是最大的概率对应的logits,那么根据上面可以得到logits[:,-1,:][100]表示的是对于词汇表第token ids为1000对应的token的概率\n",
    "- 然后经过tokenizer.decode(torch.argmax(lst)),即tokenizer.decode([100]),就是相应的token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df556e",
   "metadata": {},
   "source": [
    "### outpout_hidden_state理解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225785a",
   "metadata": {},
   "source": [
    "llm = AutoModel.from_pretrained(model_name, trust_remote_code=True,output_hidden_states=True)\n",
    "- output_hidden_states = True的理解\n",
    "- 使用output_hidden_states = True后,llm(**input),会得到三个输出\n",
    "    - last_hidden_state 最后一个hidden_states的输出\n",
    "    - past_key_values KV键值缓存,维度：(batch_size, num_heads, sequence_length, embed_size_per_head)\n",
    "    - hidden_states (output_hidden_states = True的结果) 所有隐层的输出\n",
    "    - last_hidden_state == hidden_states[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
