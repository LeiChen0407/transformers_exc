{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22fd746f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\anaconda\\envs\\llm\\lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,AutoConfig,AutoModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8bb1324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"../../model/gpt2\"\n",
    "CLLM = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "llm = AutoModel.from_pretrained(model_name, trust_remote_code=True,output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1425a26f",
   "metadata": {},
   "source": [
    "### AutoModelForCausalLM 跟 AutoModel的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab19e40",
   "metadata": {},
   "source": [
    "#### AutoModelForCausalLM 跟 AutoModel的区别\n",
    "- 多了一个MLPHead,维度为(outputsize,vocal_size)\n",
    "- 即输出到词表的映射\n",
    "\n",
    "#### gpt2属于自回归模型\n",
    "- 对于输入x,得到$y_1$,然后将[x,$y_1$]作为输入,接着得到$y_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85db5aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53031f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeacb69a",
   "metadata": {},
   "source": [
    "### greed_search vs. beam_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b80232",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "#### greed_search vs. beam_search\n",
    "- greed_search:贪心策略，对于每次的输入y,总是选择概率最大的那个,弊端：输出缺乏多样性,场景：数学(要求精确，而不是多样)\n",
    "- beam_search:束搜索，输入的时候进行束(k)展开，每次输出都进行k展开，最后计算得到最大概率的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17576636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def predict_next_tokens(model, tokenizer, sentence, num_steps=10, top_k=5):\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.clone()  \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            probs = torch.softmax(logits[:, -1, :], dim=-1, dtype=torch.float32)\n",
    "            \n",
    "            top_probs, top_indices = torch.topk(probs, top_k, dim=-1)\n",
    "            \n",
    "            step_result = {\n",
    "                '步骤': f'第{step+1}步',\n",
    "                '当前句子': tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            }\n",
    "            \n",
    "            for i in range(top_k):\n",
    "                token_id = top_indices[0][i].item()\n",
    "                probability = top_probs[0][i].item() * 100\n",
    "                token_text = tokenizer.decode([token_id])\n",
    "                \n",
    "                step_result[f'Top{i+1}_Token'] = token_text\n",
    "                step_result[f'Top{i+1}_概率(%)'] = f'{probability:.2f}%'\n",
    "            \n",
    "            results.append(step_result)\n",
    "            \n",
    "            next_token_id = top_indices[0][0].unsqueeze(0).unsqueeze(0)\n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_prediction_results(df):\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Token预测结果\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', 30)\n",
    "    \n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def run_token_prediction(model, tokenizer, sentence, num_steps=10, top_k=5):\n",
    "\n",
    "    try:\n",
    "        results_df = predict_next_tokens(model, tokenizer, sentence, num_steps, top_k)\n",
    "        formatted_df = display_prediction_results(results_df)\n",
    "        return results_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"预测过程中出现错误: {e}\")\n",
    "        return None\n",
    "\n",
    "df_result = run_token_prediction(CLLM, tokenizer, \"hello,how are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c0e38",
   "metadata": {},
   "source": [
    "### logits的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857ae47a",
   "metadata": {},
   "source": [
    "- 对任意句子(i have a )，进行tokenizer编码后(假设不包含special_token)，得到input_ids,长度为tokens_len\n",
    "- 将input_ids输入到model中,即output = model(input_ids=input_ids),\n",
    "- 得到output.logits,维度为(batch_size,tokens_len,vocab_size)\n",
    "- 对于非最后一个token,即(batch_size,tokens_len[i],vocab_size),i!=-1,输出的含义:model更加偏向对该位置token的预测,\n",
    "    - 例如(batch_size,tokens_len[0],vocab_size),是指model对第一个token(i)的预测概率的一个排序\n",
    "- 而对于最后一个token,即(batch_size,tokens_len[-1],vocab_size),输出的含义:model更加偏向对下一个token的预测\n",
    "    - 例如(batch_size,tokens_len[-1],vocab_size),更多的是对apple这个token的预测，而不是a这个token的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84598c51",
   "metadata": {},
   "source": [
    "### 相关函数解释"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b5e17",
   "metadata": {},
   "source": [
    "- torch.argsort(),返回张量排序后的索引（indices）\n",
    "    - 例如：lst = [3.1,-0.8,2.5],torch.argsort(lst),得到[1,2,0](升序顺序)\n",
    "    - torch.argsort(lst,descending=True),得到[0,2,1](降序顺序)\n",
    "- torch.argmax(),返回张量顺序后最大的索引\n",
    "    - torch.argmax(lst),得到[0]\n",
    "- model.generate()\n",
    "    - max_length:prompt+generation的长度\n",
    "    - max_new_tokens:generation的长度\n",
    "    - 默认greed_search,如果beam_num不设置的话\n",
    "- model.wte(input.input_ids)\n",
    "    - 得到每一个token的编码后的向量\n",
    "- model.wpe.weight\n",
    "    - 得到位置编码向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba778ee9",
   "metadata": {},
   "source": [
    "### 理解tokenizer.decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10718806",
   "metadata": {},
   "source": [
    "#### 如何理解tokenizer.decode()的时候，是tokenizer.decode(torch.argmax(lst)),是decode的索引,而不是logits\n",
    "- 对于model(input_ids = input_ids).logits[:,-1,:],得到的是模型对于vocab再该位置输出token的概率\n",
    "- 其中logits[:,-1,:]潜在的包含了顺序的关系，可以理解为logits[:,-1,:][0],表示的是对于词汇表token ids为0对应的token的概率\n",
    "- 然后经过torch.argmax(lst)得到最大概率的索引,假设为100,那么logits[:,-1,:][100],是最大的概率对应的logits,那么根据上面可以得到logits[:,-1,:][100]表示的是对于词汇表第token ids为1000对应的token的概率\n",
    "- 然后经过tokenizer.decode(torch.argmax(lst)),即tokenizer.decode([100]),就是相应的token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df556e",
   "metadata": {},
   "source": [
    "### outpout_hidden_state理解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225785a",
   "metadata": {},
   "source": [
    "llm = AutoModel.from_pretrained(model_name, trust_remote_code=True,output_hidden_states=True)\n",
    "- output_hidden_states = True的理解\n",
    "- 使用output_hidden_states = True后,llm(**input),会得到三个输出\n",
    "    - last_hidden_state 最后一个hidden_states的输出\n",
    "    - past_key_values KV键值缓存,维度：(batch_size, num_heads, sequence_length, embed_size_per_head)\n",
    "    - hidden_states (output_hidden_states = True的结果) 所有隐层的输出\n",
    "    - last_hidden_state == hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4ac77b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff64aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"hi, how are you?\"\n",
    "input = tokenizer(sentence, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a34437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9aa6cb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,    11,   703,   389,   345,    30]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0be5e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', '?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([15496,    11,   703,   389,   345,    30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e62690f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453],\n",
       "        [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
       "        [-0.1275,  0.0479,  0.1841,  ...,  0.0899, -0.1297, -0.0879],\n",
       "        ...,\n",
       "        [-0.0445, -0.0548,  0.0123,  ...,  0.1044,  0.0978, -0.0695],\n",
       "        [ 0.1860,  0.0167,  0.0461,  ..., -0.0963,  0.0785, -0.0225],\n",
       "        [ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings = llm.get_input_embeddings().weight\n",
    "position_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "542bffbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1518, -0.0500,  0.0857,  ...,  0.0303,  0.0272,  0.2617],\n",
       "         [ 0.0115, -0.0029,  0.0323,  ...,  0.0277, -0.0297, -0.0599],\n",
       "         [-0.0288, -0.0250,  0.0333,  ...,  0.1464,  0.1640,  0.0697],\n",
       "         [ 0.0756,  0.0559,  0.0481,  ...,  0.0595, -0.0550,  0.0846],\n",
       "         [-0.0337,  0.0484,  0.0309,  ..., -0.1242, -0.0810, -0.0539],\n",
       "         [-0.0660, -0.0374,  0.0515,  ..., -0.0804,  0.1048, -0.1505]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.wte(input.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ac19ad5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5303,   11,  703,  389,  345,   30])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c41392e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
       "          2.8267e-02,  5.4490e-02],\n",
       "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
       "          1.0172e-02, -1.5573e-04],\n",
       "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
       "          1.9325e-02, -2.1424e-02],\n",
       "        ...,\n",
       "        [-1.7987e-03,  1.6052e-03, -5.5103e-02,  ...,  1.3617e-02,\n",
       "         -7.1805e-03,  3.7552e-03],\n",
       "        [ 3.2105e-03,  1.5501e-03, -4.8944e-02,  ...,  2.0725e-02,\n",
       "         -1.1838e-02, -5.5683e-04],\n",
       "        [ 2.6610e-04,  3.0272e-03, -1.7086e-03,  ..., -4.6506e-03,\n",
       "         -2.3541e-03, -5.7855e-03]], requires_grad=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings = llm.wpe.weight \n",
    "position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bcaf2a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(5303)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3f9bc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len = input.input_ids.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c0314e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.arange(0, token_len, dtype=torch.long).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8cc2925c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(llm.wpe(position_ids)+llm.wte(input.input_ids)) == output.hidden_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1ca56e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8.8989e-06, -1.4021e-01, -2.0845e-01,  ..., -1.5329e-01,\n",
       "          -6.7826e-02, -1.9630e-01],\n",
       "         [ 4.1949e-01,  2.3525e-01,  3.4816e-01,  ...,  4.5321e-02,\n",
       "           1.5447e-01,  1.9547e-02],\n",
       "         [ 2.5089e-01, -3.9139e-01, -2.6851e-01,  ..., -3.5611e-01,\n",
       "          -1.5503e-01, -1.0117e-01],\n",
       "         [-3.7585e-02,  4.5083e-01, -6.3439e-02,  ..., -5.4199e-01,\n",
       "           2.9847e-01,  6.0528e-02],\n",
       "         [ 1.1579e-01, -3.7055e-01, -7.1209e-01,  ..., -8.2506e-02,\n",
       "           5.2692e-02,  1.6689e-01],\n",
       "         [ 2.8985e-01, -2.3842e-01,  5.7513e-02,  ..., -8.6427e-02,\n",
       "          -4.7248e-02,  3.3461e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "66dea0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8.8989e-06, -1.4021e-01, -2.0845e-01,  ..., -1.5329e-01,\n",
       "          -6.7826e-02, -1.9630e-01],\n",
       "         [ 4.1949e-01,  2.3525e-01,  3.4816e-01,  ...,  4.5321e-02,\n",
       "           1.5447e-01,  1.9547e-02],\n",
       "         [ 2.5089e-01, -3.9139e-01, -2.6851e-01,  ..., -3.5611e-01,\n",
       "          -1.5503e-01, -1.0117e-01],\n",
       "         [-3.7585e-02,  4.5083e-01, -6.3439e-02,  ..., -5.4199e-01,\n",
       "           2.9847e-01,  6.0528e-02],\n",
       "         [ 1.1579e-01, -3.7055e-01, -7.1209e-01,  ..., -8.2506e-02,\n",
       "           5.2692e-02,  1.6689e-01],\n",
       "         [ 2.8985e-01, -2.3842e-01,  5.7513e-02,  ..., -8.6427e-02,\n",
       "          -4.7248e-02,  3.3461e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d653caaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# 检查两个张量是否在容忍误差范围内相等\n",
    "is_close = torch.allclose(llm.wpe(position_ids) + llm.wte(input.input_ids), output.hidden_states[0], atol=1e-5)\n",
    "\n",
    "print(is_close)  # 返回 True 或 False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0747a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
