{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee13149",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### tokenizer理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ccc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download --resume-download google-bert/bert-base-uncased --local-dir ../model/bert-base-uncased --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0fca718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\torch2.3.1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed49910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"../model/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "sentences = \"this is a test sentence\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1ae14",
   "metadata": {},
   "source": [
    "### model\n",
    "1.模型详细结构\n",
    "- model\n",
    "\n",
    "2.模型整体结构\n",
    "- model.config\n",
    "\n",
    "3.模型参数\n",
    "- model.num_parameters()\n",
    "- eps 相当于$\\epsilon$\n",
    "- elementwise_affine 相当于bias\n",
    "$$\\mathrm{output}=\\mathrm{weight}\\cdot\\frac{\\mathrm{input}-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}+\\mathrm{bias}$$\n",
    "\n",
    "### model(**tokens)\n",
    "1.model(**tokens) 解析\n",
    "- 输出得到last_hidden_state、pooler_output\n",
    "- last_hidden_state维度:(batch_size,sequence_length,hidden_size)\n",
    "- pooler_output维度:(batch_size,hidden_size)\n",
    "- 可将两者用于下游任务，例如分类等任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65166e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231, 6251,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(sentences, truncation = True,padding=True,max_length=256,return_tensors=\"pt\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fde148c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'this', 'is', 'a', 'te', '##set', 'sentence', '[SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(sentences)\n",
    "tokenizer.encode(sentences)\n",
    "tokenizer.decode([101, 2023, 2003, 1037, 8915, 13462, 6251, 102])\n",
    "tokenizer.convert_ids_to_tokens([101, 2023, 2003, 1037, 8915, 13462, 6251, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f44ea9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1037, 3231, 6251, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca53eeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2023, 2003, 1037, 3231, 6251]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids([ 'this', 'is', 'a', 'test',  'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adfb7345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2023, 2003, 1037, 3231, 6251]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(sentences.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "401d69a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map.values()\n",
    "tokenizer.convert_tokens_to_ids(tokenizer.special_tokens_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0bdd127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff06790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**tokens)\n",
    "with torch.no_grad():\n",
    "    output = model(**tokens)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89d7d7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9c6c66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[0.0987, 0.1810]]), hidden_states=None, attentions=None)\n",
      "tensor([[0.4794, 0.5206]])\n",
      "tensor([[0.4794, 0.5206]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "sequence_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = sequence_model(**tokens)\n",
    "    softmax = output.logits.softmax(dim=-1)\n",
    "    test = torch.softmax(output.logits, dim=-1)\n",
    "    \n",
    "    print(output)\n",
    "    print(softmax)\n",
    "    print(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547a0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8247e5a6",
   "metadata": {},
   "source": [
    "### tokenizer\n",
    "1.查看分词情况:\n",
    "- tokenizer.tokenize(sentences)\n",
    "\n",
    "2.编码解码\n",
    "- tokenizer.encode(sentences) = tokenizer.convert_tokens_to_ids()\n",
    "- tokenizer.decode(senteencs) = tokenizer.convert_ids_to_tokens()\n",
    "\n",
    "3.特殊编码\n",
    "- tokenizer.special_tokens_map\n",
    "\n",
    "4.词汇表\n",
    "- tokenizer.vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.3.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
